{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge/Pull Request Checklist\n",
    "\n",
    "* For the csv files in the **Resources** folder, dont commit the zip files or the extracted csv files.\n",
    "* Make sure to restart and clear output in jupyter notebook.\n",
    "* Code is documented and includes comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks and Timeline\n",
    "\n",
    "* Update the status of the tasks on the team [Trello Board](https://trello.com/b/qjMY63WI/whos-doing-what) so we know who is working on what."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started and Setup\n",
    "\n",
    "* **Team Lead** for this section: Phil\n",
    "* Before running these cells, you will need an API key for using YouTube API v3.\n",
    "    * If you already have a Google API key, you can use that one or create a new one from the Google Cloud Console.\n",
    "    * Instructions for creating an API key and enabling YouTube API v3 are in the [README file](./README.md).\n",
    "    * After you have your API key, create a file called **config.py** in the project root directory (**team_hopper**) where you will add the key.\n",
    "* After you have your API key set up, run the cells in this section to set up the project locally on your computer.\n",
    "* Running these cells will:\n",
    "    * Import the necessary dependencies, including reading the YouTube API key from the config.py file.\n",
    "    * Extract the data zip files in the **Resources** folder, which contains all of the csv files needed for this project.\n",
    "    * Import the csv files into this notebook.\n",
    "    * Read the csv files into pandas dataframes.\n",
    "* Questions or issues with the setup instructions, ask phil.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependencies used for this project.\n",
    "import pandas as pd\n",
    "import requests\n",
    "from config import youtube_api_key\n",
    "from pprint import pprint\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os, zipfile\n",
    "import shutil\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the country codes to instruct the YouTube API\n",
    "# to return the list of video categories available in the specified country.\n",
    "# These values are ISO 3166-1 alpha-2 country codes.\n",
    "country_codes = [\"US\", \"GB\", \"CA\", \"DE\", \"FR\", \"AU\", \"IE\",\"IN\", \"JP\", \"KR\", \"MX\", \"RU\", \"ES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this cell will unzip the data files in the Resources folder for you.\n",
    "extension = \".zip\"\n",
    "extracted_dir_name = \"youtube_trending\"\n",
    "\n",
    "# Get the current working directory..\n",
    "# You need to be in the root directory of this project (same directory as this notebook) for this to work properly.\n",
    "cwd_dir_name = os.getcwd()\n",
    "print(f\"The current working directory is {cwd_dir_name}.\")\n",
    "\n",
    "os.chdir(\"Resources\") # change directory from working dir to dir with the zip file(s) .\n",
    "# This should be your \"Resources\" folder.\n",
    "dir_name = os.getcwd()\n",
    "print(f\"You are now in the following directory: {dir_name}.\")\n",
    "\n",
    "for item in os.listdir(dir_name): # loop through the items in the directory.\n",
    "    if item.endswith(extension): # check for \".zip\" extension\n",
    "        if item == \"youtube_trending.zip\":\n",
    "            extracted_dir_name = \"youtube_trending\"\n",
    "        if item == \"trending_videos_2020.zip\":\n",
    "            extracted_dir_name = \"trending_videos_2020\"\n",
    "        try:\n",
    "            file_name = os.path.abspath(item) # get full path of files\n",
    "            zip_ref = zipfile.ZipFile(file_name) # create zipfile object\n",
    "            # Check if the directory where we plan to extract the files already exists or not.\n",
    "            if not os.path.exists(extracted_dir_name):\n",
    "                os.mkdir(extracted_dir_name) # make a directory where the zip files will be extracted.\n",
    "            unzipped_directory = os.path.join(extracted_dir_name) # reference to the directory where the zip files will be extracted.\n",
    "            zip_ref.extractall(unzipped_directory) # extract file to dir\n",
    "            zip_ref.close() # close file\n",
    "            print(f\"Successfully unzipped youtube data files into the following folder: {unzipped_directory} inside of {dir_name}.\")\n",
    "        except:\n",
    "            print(f\"Error trying to unzip youtube data file(s).\")\n",
    "            \n",
    "# Go up one directory into the project root directory.\n",
    "os.chdir(os.path.normpath(os.getcwd() + os.sep + os.pardir))\n",
    "print(os.path.normpath(os.getcwd() + os.sep + os.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the csv files from the trending youtube video statistics kaggle dataset.\n",
    "path_to_youtube_trending_csvs = os.path.join(\".\", \"Resources\", \"youtube_trending\")\n",
    "all_files = glob.glob(os.path.join(path_to_youtube_trending_csvs, \"*.csv\"))\n",
    "\n",
    "df_from_each_file = []\n",
    "\n",
    "for f in all_files:\n",
    "    filename = os.path.basename(f)\n",
    "    df_country = pd.read_csv(f, encoding =\"ISO-8859-1\")\n",
    "    df_country[\"Country\"] = f\"{filename[0]}{filename[1]}\"\n",
    "    df_from_each_file.append(df_country)\n",
    "\n",
    "# Concantenated dataframe that contains all countries.\n",
    "# Can filter list by country using the \"Country\" column\n",
    "trending_videos_concatenated_df = pd.concat(df_from_each_file, ignore_index=True)\n",
    "trending_videos_concatenated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to get the list of video categories from the YouTube API,\n",
    "# which can be associated with youtube videos by category id.\n",
    "def getVideoCategories(country_code):    \n",
    "    base_url_categories = \"https://www.googleapis.com/youtube/v3/videoCategories\"\n",
    "    part_categories = \"snippet\"\n",
    "    query_url_categories = f\"{base_url_categories}?part={part_categories}&regionCode={country_code}&key={youtube_api_key}\"\n",
    "    categories_response = requests.get(query_url_categories).json()\n",
    "    category_items = categories_response[\"items\"]\n",
    "    categories = []\n",
    "    \n",
    "    for category in category_items:\n",
    "        categories_dict = {}\n",
    "        categories_dict[\"category_id\"] = category[\"id\"]\n",
    "        categories_dict[\"channel_id\"] = category[\"snippet\"][\"channelId\"]\n",
    "        categories_dict[\"title\"] = category[\"snippet\"][\"title\"]\n",
    "        categories.append(categories_dict)\n",
    "    return categories\n",
    "\n",
    "for country in country_codes:\n",
    "    categories = []\n",
    "    categories = getVideoCategories(country)\n",
    "    categories_df = pd.DataFrame(categories)\n",
    "    output_file = f\"{country}_categories.csv\"\n",
    "    output_dir = Path(\"./Resources/categories\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    categories_df.to_csv(output_dir / output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the csv files that list the different categories.\n",
    "path_to_categories_csvs = os.path.join(\".\", \"Resources\", \"categories\")\n",
    "all_category_files = glob.glob(os.path.join(path_to_categories_csvs, \"*.csv\"))\n",
    "\n",
    "df_from_each_categories_file = []\n",
    "\n",
    "for f in all_category_files:\n",
    "    filename = os.path.basename(f)\n",
    "    df_categories = pd.read_csv(f, encoding =\"ISO-8859-1\")\n",
    "    df_categories[\"Country\"] = f\"{filename[0]}{filename[1]}\"\n",
    "    df_from_each_categories_file.append(df_categories)\n",
    "\n",
    "# Concantenated dataframe that contains all categories\n",
    "# Can filter list by country using the \"Country\" column\n",
    "categories_concatenated_df = pd.concat(df_from_each_categories_file, ignore_index=True)\n",
    "\n",
    "# Merge the dataframe of trending videos with the dataframe of categories on category_id and on country.\n",
    "merged_trending_df = pd.merge(trending_videos_concatenated_df, categories_concatenated_df,  how='left', left_on=['category_id','Country'], right_on = ['category_id','Country'], suffixes=(\"_video\", \"_category\"))\n",
    "\n",
    "merged_trending_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These csv files are YouTube's most popular videos for 2020.\n",
    "\n",
    "# Path to the csv files.\n",
    "path_to_trending_2020_csvs = os.path.join(\".\", \"Resources\", \"trending_videos_2020\")\n",
    "all_files_2020 = glob.glob(os.path.join(path_to_trending_2020_csvs, \"*.csv\"))\n",
    "\n",
    "df_from_each_file_2020 = []\n",
    "\n",
    "for f in all_files_2020:\n",
    "    filename = os.path.basename(f)\n",
    "    df_country = pd.read_csv(f, encoding =\"ISO-8859-1\")\n",
    "    df_country[\"Country\"] = f\"{filename[0]}{filename[1]}\"\n",
    "    df_from_each_file_2020.append(df_country)\n",
    "\n",
    "# Concantenated dataframe that contains all countries.\n",
    "# Can filter list by country using the \"Country\" column\n",
    "trending_2020_concatenated_df = pd.concat(df_from_each_file_2020, ignore_index=True)\n",
    "trending_2020_concatenated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the csv files that list the different categories.\n",
    "path_to_categories_csvs = os.path.join(\".\", \"Resources\", \"categories\")\n",
    "all_category_files = glob.glob(os.path.join(path_to_categories_csvs, \"*.csv\"))\n",
    "\n",
    "df_from_each_categories_file = []\n",
    "\n",
    "for f in all_category_files:\n",
    "    filename = os.path.basename(f)\n",
    "    df_categories = pd.read_csv(f, encoding =\"ISO-8859-1\")\n",
    "    df_categories[\"Country\"] = f\"{filename[0]}{filename[1]}\"\n",
    "    df_from_each_categories_file.append(df_categories)\n",
    "\n",
    "# Concantenated dataframe that contains all categories\n",
    "# Can filter list by country using the \"Country\" column\n",
    "categories_concatenated_df = pd.concat(df_from_each_categories_file, ignore_index=True)\n",
    "\n",
    "# Merge the dataframe of trending videos with the dataframe of categories on category_id and on country.\n",
    "merged_trending_2020_df = pd.merge(trending_2020_concatenated_df, categories_concatenated_df,  how='left', left_on=['category_id','Country'], right_on = ['category_id','Country'], suffixes=(\"_video\", \"_category\"))\n",
    "\n",
    "merged_trending_2020_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving a list of YouTube's most popular videos\n",
    "\n",
    "* You do **NOT** need to run this to set up the repository.\n",
    "* The following function retrieves a list of YouTube's most popular videos using version 3 of the YouTube API.\n",
    "* The API is updated daily to return the list of trending videos, which can be found on YouTube's site [here](https://www.youtube.com/feed/trending).\n",
    "* The function takes a country code as input, which identifies the country for which you are retrieving videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrendingVideos(country_code):\n",
    "    # The base api url for the youtube data api.\n",
    "    base_url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "\n",
    "    # The page token identifies a specific page the API should return.\n",
    "    next_page_token=\"&\"\n",
    "\n",
    "    # Comma separated list of one or more video resource properties that the API response will include.\n",
    "    part = \"snippet,contentDetails,statistics\"\n",
    "\n",
    "    # The chart that you want to retrieve.\n",
    "    # mostPopular - returns the most popular (trending) videos.\n",
    "    chart = \"mostPopular\"\n",
    "\n",
    "    # The max results that should be returned in the list. Can return up to 50 results per page.\n",
    "    max_results = 50\n",
    "\n",
    "    # Create variable to store list of trending videos.\n",
    "    videos = []\n",
    "\n",
    "    while next_page_token is not None:\n",
    "        print(f\"One sec... getting trending videos for {country_code}....\")\n",
    "        query_url = f\"{base_url}?part={part}{next_page_token}chart={chart}&key={youtube_api_key}&maxResults={max_results}&regionCode={country_code}\"\n",
    "        trending_videos_response = requests.get(query_url).json()\n",
    "        trending_videos = trending_videos_response[\"items\"]\n",
    "        for video in trending_videos:\n",
    "            snippet = video[\"snippet\"]\n",
    "            contentDetails = video[\"contentDetails\"]\n",
    "            statistics = video[\"statistics\"]\n",
    "\n",
    "            video_dict = {}\n",
    "\n",
    "             # Fetch the id of the video.\n",
    "            video_dict[\"video_id\"] = video[\"id\"]\n",
    "\n",
    "            # The date the video was on youtube's trending list.\n",
    "            video_dict[\"trending_date\"] = time.strftime(\"%y.%d.%m\")\n",
    "\n",
    "            # Fetch video content details\n",
    "            # duration - the property value is an ISO 8601 duration. \n",
    "            video_dict[\"duration\"] = contentDetails[\"duration\"]\n",
    "            video_dict[\"captions_available\"] = contentDetails[\"caption\"]\n",
    "\n",
    "            # Fetch basic details about the video (snippet).\n",
    "            video_dict[\"title\"] = snippet[\"title\"]\n",
    "            video_dict[\"description\"] = snippet[\"description\"]\n",
    "            video_dict[\"publish_time\"] = snippet[\"publishedAt\"]\n",
    "            video_dict[\"category_id\"] = snippet[\"categoryId\"]\n",
    "            video_dict[\"channel_id\"] = snippet[\"channelId\"]\n",
    "            video_dict[\"channel_title\"] = snippet[\"channelTitle\"]\n",
    "            video_dict[\"localized_description\"] = snippet[\"localized\"][\"description\"]\n",
    "            video_dict[\"localized_title\"] = snippet[\"localized\"][\"title\"]\n",
    "            video_dict[\"live_broadcast_content\"] = snippet[\"liveBroadcastContent\"]\n",
    "            try:\n",
    "                video_dict[\"tags\"] = snippet[\"tags\"]\n",
    "            except KeyError:\n",
    "                video_dict[\"tags\"] = []\n",
    "            video_dict[\"thumbnail_link\"] = snippet[\"thumbnails\"][\"default\"][\"url\"]\n",
    "\n",
    "            # Fetch video statistics.\n",
    "            video_dict[\"comments_disabled\"] = False\n",
    "            try:\n",
    "                video_dict[\"comment_count\"] = statistics[\"commentCount\"]\n",
    "            except KeyError:\n",
    "                video_dict[\"comment_count\"] = 0\n",
    "                video_dict[\"comments_disabled\"] = True\n",
    "            ratings_disabled = False\n",
    "            try:\n",
    "                video_dict[\"dislikes\"] = statistics[\"dislikeCount\"] \n",
    "                video_dict[\"likes\"] = statistics[\"likeCount\"]\n",
    "            except KeyError:\n",
    "                video_dict[\"dislikes\"] = 0\n",
    "                video_dict[\"likes\"] = 0\n",
    "                ratings_disabled = True\n",
    "            video_dict[\"favorites\"] = statistics[\"favoriteCount\"]    \n",
    "            video_dict[\"views\"] = statistics[\"viewCount\"]\n",
    "\n",
    "            videos.append(video_dict)\n",
    "\n",
    "        # Check the nextPageToken on the API response to see if there is another page to fetch data from.\n",
    "        try:\n",
    "            next_page_token = trending_videos_response[\"nextPageToken\"]\n",
    "            next_page_token = f\"&pageToken={next_page_token}&\"\n",
    "        except KeyError:\n",
    "            next_page_token = None\n",
    "            \n",
    "    return videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this cell retrieves the most popular videos for each country using the YouTube API and stores the output\n",
    "# in the Resources folder.\n",
    "# You do NOT need to run this cell.\n",
    "for country in country_codes:\n",
    "    country_videos = []\n",
    "    country_videos = getTrendingVideos(country)\n",
    "    country_videos_df = pd.DataFrame(country_videos)\n",
    "#   Leave the below lines commented out.\n",
    "#   output_file = f\"{country}_videos.csv\"\n",
    "#   output_dir = Path(\"./Resources/trending_videos_2020\")\n",
    "#   output_dir.mkdir(parents=True, exist_ok=True)\n",
    "#   country_videos_df.to_csv(output_dir / output_file, index=False, header=False, mode=\"a\")\n",
    "\n",
    "country_videos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell zips up the csv data files in \"Resources/trending_videos_2020\"\n",
    "# You do NOT need to run this.\n",
    "# dir_name = Path(\"./Resources/trending_videos_2020\")\n",
    "# shutil.make_archive(\"./Resources/trending_videos_2020\", 'zip', dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Data\n",
    "\n",
    "* **Team Lead** for this section: Jenna\n",
    "* Check for null/na values. Remove (if necessary).\n",
    "* Rename columns to be something more meaningful (remove underscores from column names).\n",
    "    * For example, change \"category_id\" to \"Category ID\".\n",
    "* Remove unnecessary columns.\n",
    "* As an additional resource, check out [this file](https://umn.bootcampcontent.com/University-of-Minnesota-Boot-Camp/UofM-STP-DATA-PT-11-2019-U-C/blob/master/04-Pandas/Activities/2019-12-16_and_17_Pandas_lesson_2/03-Ins_CleaningData/Solved/CleaningData.ipynb).\n",
    "* Do anything else that you think will make the data easy to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_trending_df = merged_trending_df[['video_id', 'trending_date', 'title_video', 'channel_title', \n",
    "                                        'publish_time', 'tags', 'views', 'likes', 'dislikes', \n",
    "                                        'Country', 'title_category']]\n",
    "clean_trending_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_trending_df = clean_trending_df.rename(columns = \n",
    "                                             {\"video_id\": \"Video ID\", \n",
    "                                              \"trending_date\": \"Trending Date\",\n",
    "                                             \"title_video\": \"Video Title\", \n",
    "                                             \"channel_title\":\"Channel Title\", \n",
    "                                             \"publish_time\": \"Publish Time\",\n",
    "                                             \"tags\": \"Tags\",\n",
    "                                             \"views\": \"Views\", \n",
    "                                              \"likes\": \"Likes\",\n",
    "                                             \"dislikes\": \"Dislikes\",\n",
    "                                             \"description\": \"Description\", \n",
    "                                             \"title_category\": \"Category\"})\n",
    "clean_trending_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look for N/As\n",
    "clean_trending_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_trending_df['Category'].unique()\n",
    "#if time figure out what nan is\n",
    "clean_trending_df.dropna(how='any')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_trending_df['Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_concatenated_df['title'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "\n",
    "* **Team Lead** for this section: Katrina\n",
    "* Perform a .describe() on the dataframe to get some quick summary statistics.\n",
    "* Calculate the mean, median, standard deviation, variance, and standard error of mean (sem) for trending videos.\n",
    "  * Do this for the numeric columns: views, likes, and comments.\n",
    "  * Might be also a good idea to group the dataframe by category id and calculate those same statistics.\n",
    "* As an additional resource, check out [this file](https://umn.bootcampcontent.com/University-of-Minnesota-Boot-Camp/UofM-STP-DATA-PT-11-2019-U-C/blob/master/05-Matplotlib/Activities/2020-01-06_and_07_lesson_3/01-Ins_Summary_Statistics/Solved/samples.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
